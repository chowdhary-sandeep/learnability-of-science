{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import numpy as np\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0    # Distance of left side of character from left side of page.\n",
    "x1 = 0.4  # Distance of right side of character from left side of page.\n",
    "y0 = 0  # Distance of bottom of character from bottom of page.\n",
    "y1 = 1  # Distance of top of character from bottom of page.\n",
    "\n",
    "\n",
    "# file = pdfplumber.open('Kenneth M Hoffman, Ray Kunze - Linear Algebra -Prentice Hall (1971).pdf')\n",
    "file = pdfplumber.open('Linear Algebra, 2Nd Edition - Kenneth Hoffmann And Ray Kunze.pdf')\n",
    "\n",
    "node_list = []\n",
    "\n",
    "for x in range(400,415):\n",
    "    all_content = []\n",
    "    page = file.pages[x]\n",
    "    \n",
    "    width = page.width\n",
    "    height = page.height\n",
    "\n",
    "    # Crop pages\n",
    "    left_bbox = (0*float(width), y0*float(height), x1*float(width), y1*float(height))\n",
    "    page_crop = page.crop(bbox=left_bbox)\n",
    "    left_text = page_crop.extract_text()\n",
    "\n",
    "    right_bbox = (x1*float(width), y0*float(height), 1*float(width), y1*float(height))\n",
    "    page_crop = page.crop(bbox=right_bbox)\n",
    "    right_text = page_crop.extract_text()\n",
    "    page_context = '\\n'.join([left_text, right_text])\n",
    "    all_content.append(page_context)\n",
    "\n",
    "    text_=all_content#.extract_text()\n",
    "\n",
    "    if 'Index' in text_[0]:\n",
    "        new_text_ = text_[0].split(',')\n",
    "        new_text_ = list(new_text_)\n",
    "\n",
    "        for y in range(len(new_text_)):\n",
    "            if new_text_[y].isdigit():\n",
    "                pass\n",
    "            else:\n",
    "                result = ''.join([i for i in new_text_[y] if not i.isdigit()])\n",
    "                node_list.append(result.replace('\\n',''))\n",
    "\n",
    "node_list = [name for name in node_list if name.strip()]\n",
    "node_list = [name.lstrip() for name in node_list]\n",
    "node_list = [name for name in node_list if name[0].isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,node in enumerate(node_list):\n",
    "    node = ' '.join( [w for w in node.split() if len(w)>1] )\n",
    "    node = node.split(':')\n",
    "    node = node[0].replace('Index ','')\n",
    "    node_list[i] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_sections=[]\n",
    "\n",
    "for x in range(5,8):\n",
    "    \n",
    "    text_ = file.pages[x].extract_text()\n",
    "    text_list = text_.split('\\n')\n",
    "    text_list_new = []\n",
    "    for line_ in range(len(text_list)):\n",
    "        \n",
    "        text=text_list[line_]\n",
    "        \n",
    "        if (text[0].isdigit()) and (text.rstrip()[-1].isdigit()):\n",
    "            text_list_new.append(text)\n",
    "        elif (text[0].isdigit()) and ((line_+1)<=(len(text_list)-1)):\n",
    "            text_list_new.append(text+text_list[line_+1])\n",
    "#         if '8.4' in text:\n",
    "#             print(text,'\\n conditions-\\n',text[0].isdigit() and text.rstrip()[-1].isdigit(),'\\n',text+text_list[line_+1])\n",
    "#             print(text_list[line_+1])\n",
    "            \n",
    "#     text_list_new = [text.rstrip('1234567890') for text in text_list_new]\n",
    "    text_list_new=[x for x in text_list_new if len(x)>5]\n",
    "    all_sections=all_sections+(text_list_new)\n",
    "all_sections=[x[0:len(x)-4].lower().rstrip() for x in all_sections]\n",
    "# all_sections=[x.split('.')[-1].lstrip() for x in all_sections]\n",
    "all_sections= [re.sub('\\s+',' ',x) for x in all_sections]\n",
    "all_sections=[x.split('.')[-1].lstrip() for x in all_sections]\n",
    "all_sections= [re.sub(r'[0-9]+', '', x) for x in all_sections]\n",
    "all_sections=[x.lstrip() for x in all_sections]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 matrices and elementary\n",
      "matrices and elementary row operations\n"
     ]
    }
   ],
   "source": [
    "all_sections=[x.split('.')[-1].lstrip() for x in all_sections]\n",
    "\n",
    "it_section=0\n",
    "dict_section_page_line={}\n",
    "\n",
    "for x in range(8,394):\n",
    "    text_=file.pages[x].extract_text()\n",
    "    text_=text_.lower()\n",
    "    text_list = text_.split('\\n')\n",
    "#     x=text_list[1]\n",
    "    xx=[x.split('.') for x in text_list]\n",
    "    text_new=[y[-1].rstrip().lstrip() for y in xx]\n",
    "    text_new=[re.sub('\\s+',' ',x) for x in text_new]\n",
    "    line_num=np.where(all_sections[it_section]== np.array(text_new))[0]\n",
    "    \n",
    "    \n",
    "    if line_num.shape[0]>0:\n",
    "#         print(it_section)\n",
    "        dict_section_page_line[str(it_section)+'.'+all_sections[it_section]]= [x,line_num]\n",
    "        it_section=it_section+1\n",
    "    else:\n",
    "        for it__ in range(len(text_new)-2):\n",
    "            joined_=text_new[it__]+' '+text_new[it__+1]\n",
    "            if x==13:\n",
    "                print(joined_)\n",
    "            if re.sub('\\s+',' ',joined_)==all_sections[it_section]:\n",
    "                line_num=it__\n",
    "                dict_section_page_line[str(it_section)+'.'+all_sections[it_section]]= [x,line_num]\n",
    "                it_section=it_section+1\n",
    "                break\n",
    "        for it__ in range(len(text_new)-1):\n",
    "            joined_=text_new[it__]\n",
    "#             if x==13:\n",
    "#                 print(joined_)\n",
    "            if re.sub('\\s+',' ',joined_).replace(' ','')==all_sections[it_section].replace(' ',''):\n",
    "                line_num=it__\n",
    "                dict_section_page_line[str(it_section)+'.'+all_sections[it_section]]= [x,line_num]\n",
    "                it_section=it_section+1\n",
    "                break\n",
    "\n",
    "    if it_section==len(all_sections):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_section_page_line.keys():\n",
    "    if type(dict_section_page_line[key][1])==np.ndarray:\n",
    "\n",
    "        dict_section_page_line[key][1]=dict_section_page_line[key][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_section_text={}\n",
    "page_nums=[]\n",
    "for key in dict_section_page_line.keys():\n",
    "    page_nums.append(dict_section_page_line[key][0])\n",
    "text_section=[]    \n",
    "section_names=list(dict_section_page_line.keys())\n",
    "it_section=0\n",
    "for x in range(8,394):\n",
    "    \n",
    "    text_=file.pages[x].extract_text()\n",
    "    text_=text_.lower()\n",
    "    text_list = text_.split('\\n')\n",
    "    \n",
    "    if (x in page_nums) and x>8:\n",
    "        it_section=it_section+1\n",
    "        key=section_names[it_section]\n",
    "#         print('line_num-----',dict_section_page_line[key][1])\n",
    "        text_section.append(text_list[1:dict_section_page_line[key][1]])\n",
    "        dict_section_text[section_names[it_section-1]]=text_section\n",
    "        text_section=text_list[dict_section_page_line[key][1]:-1]\n",
    "    else:\n",
    "        text_section.append(text_list[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_section_textcombined=dict_section_text.copy()\n",
    "for k in dict_section_text.keys():\n",
    "#     print(dict_section_text[k],'\\n\\n')\n",
    "    text_combined='  '\n",
    "    y=' '\n",
    "    \n",
    "    for xx in dict_section_text[k]:\n",
    "        for x in xx:\n",
    "            if text_combined[-2]=='-':\n",
    "                text_combined=text_combined[0:-2]+x\n",
    "            else:\n",
    "                text_combined=text_combined+x\n",
    "    dict_section_textcombined[k]=text_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_section_keywords_line_no = {keyword: {section: [line_numbers_within_section]}}\n",
    "def dict_text_to_dict_keyword(temp_dict_text):\n",
    "\n",
    "    temp_dict_section_keywords_line_no={}\n",
    "\n",
    "    glossary_words=node_list.copy()\n",
    "    it_gloss=0\n",
    "    for x in glossary_words:\n",
    "        print(it_gloss,'==',x,end='\\r')\n",
    "        it_gloss=it_gloss+1\n",
    "        x_key=x\n",
    "        x=x.split(' ')\n",
    "\n",
    "        temp_dict_section_keywords_line_no[x_key]={}\n",
    "\n",
    "        for k in temp_dict_text.keys():\n",
    "            temp_dict_section_keywords_line_no[x_key][k]=[]\n",
    "            search_text=temp_dict_text[k]\n",
    "            search_text=[x for x in search_text.split(' ') if x!='']\n",
    "            for it in range(len(search_text)-1):\n",
    "                if x_key == search_text[it:it+len(x_key)]:\n",
    "                    (temp_dict_section_keywords_line_no[x_key][k]).append(it)\n",
    "\n",
    "    return temp_dict_section_keywords_line_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the section text dictionary into two parts -- only section and only exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_only_section_textcombined, dict_only_exercise_textcombined = (dict_section_textcombined.copy() for x in range(2))\n",
    "\n",
    "for key, value in dict_section_textcombined.items():\n",
    "    full_text = dict_section_textcombined[key]\n",
    "    \n",
    "    if 'exercises 1.' in full_text:\n",
    "        split_text = full_text.split('exercises 1.',1)\n",
    "        dict_only_section_textcombined[key] = split_text[0]\n",
    "        dict_only_exercise_textcombined[key] = split_text[1]\n",
    "\n",
    "    else:\n",
    "        dict_only_exercise_textcombined[key] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 == Zero matrixrior) productsformationionnequations) Linear functional\r"
     ]
    }
   ],
   "source": [
    "dict_section_keywords_line_no = dict_text_to_dict_keyword(dict_section_textcombined)\n",
    "dict_only_section_keywords_line_no = dict_text_to_dict_keyword(dict_only_section_textcombined)\n",
    "dict_only_exercise_keywords_line_no = dict_text_to_dict_keyword(dict_only_exercise_textcombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keywords = rnd.choices(list(dict_section_keywords_line_no.keys()),k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Left inverse',\n",
       " 'Relation',\n",
       " 'Jordan form of',\n",
       " 'Jordan matrix',\n",
       " 'Algebraically closed field',\n",
       " 'Independence',\n",
       " 'Cyclic',\n",
       " 'Eigenvalue (see Characteristic',\n",
       " 'Equiva',\n",
       " 'Nullity of linear transformation',\n",
       " 'Complementary subspace',\n",
       " 'Disjoint subspaces (see Independent',\n",
       " 'Grassman ring',\n",
       " 'Approximation',\n",
       " 'T-annihilator',\n",
       " 'Module',\n",
       " 'Independence',\n",
       " 'Linearly dependent (independent)',\n",
       " 'Standard basis of F‚Äù',\n",
       " 'Derivative of polynomial']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    if np.sum(list((dict_section_keywords_line_no[plot_keywords[0]]).values())) != 0:\n",
    "        print('yay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "507645663622515d8257022cf03b5024c7bdb8862f4076706ebffba46bdb73d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
